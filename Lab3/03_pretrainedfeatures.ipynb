{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will load an audio research dataset, and analyse it using spectrogram-based audio features, as well as pre-trained deep learning features.\n",
    "\n",
    "If you want to work on your PC, install the python package dependencies. Here's how on Debian or Ubuntu, using the terminal *command line*:\n",
    "\n",
    "First we create a \"virtual environment\" to install packages without altering your core Python install. This is optional but highly recommended:\n",
    "\n",
    "```\n",
    "python3 -m venv venv\n",
    "source activate venv/bin/activate\n",
    "```\n",
    "\n",
    "If you do the above, your Jupyter list of python kernels will have a new entry `venv_vggish`. When you launch Jupyter, you need to select that as the kernel used to run your code.\n",
    "Go to the menu option **Kernel>Change kernel>venv_vggish**.\n",
    "\n",
    "These are the **required** terminal commands to install needed packages:\n",
    "\n",
    "```\n",
    "# now install the modules\n",
    "source activate venv/bin/activate\n",
    "pip install numpy scipy\n",
    "pip install librosa matplotlib seaborn jupyter\n",
    "pip install torch\n",
    "```\n",
    "\n",
    "If you used a virtualenv, then after installing all the packages you should register your virtualenv to be available within the notebook:\n",
    "\n",
    "```\n",
    "python -m ipykernel install --user --name=venv_vggish\n",
    "```\n",
    "\n",
    "Download **torchvggish** package from here https://github.com/harritaylor/torchvggish and put it into your workspace. You don't have to do it if you work on jhub since I already included it in the `Lab3` folder,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvggish\n",
    "import torch\n",
    "from torchvggish import vggish_input, vggish, vggish_params, mel_features\n",
    "from torchvggish import vggish\n",
    "from hubconf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we'll use\n",
    "\n",
    "import numpy as np\n",
    "import os, glob, csv\n",
    "\n",
    "# librosa is a widely-used audio processing library\n",
    "import librosa\n",
    "\n",
    "# torchvggish gives us access to pretrained vggish within PyTorch\n",
    "from torchvggish import vggish, vggish_input\n",
    "\n",
    "# sklearn is a widely-used machine learning library\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you work on your PC, if you haven't already done so, download the dataset. It is called \"warblrb10k\" and was created as part of the [Bird Audio Detection challenge](https://doi.org/10.1111/2041-210X.13103).\n",
    "\n",
    "Don't click these links unless you've the bandwidth to download!\n",
    "\n",
    "* The [WAV audio data](https://archive.org/download/warblrb10k_public/warblrb10k_public_wav.zip) are 4.6 GB. Beware - this could take hours to download on a slow connection.\n",
    "* The [CSV annotation data](https://figshare.com/articles/Bird_Audio_Detection_Challenge_2016_public_data/3851466) are 200 kB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a85978657a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# USER CONFIGURATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Please alter the paths here to where the data are stored on your local filesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbinarylabelcsv\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/shared_storage/ECS7013P/bird_audio_detection/warblrb10k_public_metadata_2018.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maudiofilefolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/shared_storage/ECS7013P/warblrb10k_public_wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# USER CONFIGURATION\n",
    "# Please alter the paths here to where the data are stored on your local filesystem\n",
    "binarylabelcsv  = os.path.expanduser(\"~/shared_storage/ECS7013P/bird_audio_detection/warblrb10k_public_metadata_2018.csv\")\n",
    "audiofilefolder = os.path.expanduser(\"~/shared_storage/ECS7013P/warblrb10k_public_wav\")\n",
    "\n",
    "maxfilestoload  = 100      # limit, because loading the whole dataset is v slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load the metadata labels\n",
    "binarylabels = {}\n",
    "with open(binarylabelcsv, 'r') as infp:\n",
    "        rdr = csv.DictReader(infp)\n",
    "        for row in rdr:\n",
    "                binarylabels[row['itemid']] = int(row['hasbird'])\n",
    "                if len(binarylabels)==maxfilestoload:\n",
    "                        break  # note, here we are restricting the maximum number of rows.\n",
    "\n",
    "fkeys = sorted(binarylabels.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load the WAV audio data and convert each file to mel-spectrogram features. We can use `librosa` to do this (in general, that's a good idea). However, later we're going to be using the \"VGGish\" neural net to analyse data, and in the VGG paper the researchers describe their mel-spectrogram implementation, which is slightly different. So, we will instead use a feature extraction function from the `torchvggish` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the audio data and extract mel spectrograms using the function of the torchvggish package\n",
    "# WARNING: this will be SLOW if maxfilestoload is a large number. (50 files takes about 20 seconds)\n",
    "melspecs = {itemid:vggish_input.wavfile_to_examples(\"%s/%s.wav\" % (audiofilefolder, itemid))\n",
    "    for itemid, hasbird in binarylabels.items()}\n",
    "\n",
    "# There may be some unusual entries with very short audio.\n",
    "# If so we will delete them from our data in memory, since we cannot analyse them usefully.\n",
    "for itemid in list(binarylabels.keys()):\n",
    "        if melspecs[itemid].shape[0] == 0:\n",
    "                print(\"Deleting zero-length entry %s\" % (itemid))\n",
    "                del melspecs[itemid]\n",
    "                del binarylabels[itemid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the data structure we have for one example\n",
    "example = melspecs[fkeys[0]]\n",
    "\n",
    "# Observe that \"example\" is NOT a numpy array. It's a Torch-specific data type\n",
    "print(\"example.shape:\")\n",
    "print(example.shape)\n",
    "\n",
    "# Here's how you convert it to a numpy array:\n",
    "example_np = example.detach().numpy()\n",
    "print(\"example_np.shape:\")\n",
    "print(example_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"example\" is a four-dimensional tensor.\n",
    "\n",
    "(10, 1, 96, 64)\n",
    "\n",
    "QUESTION 1: What do these four dimensions represent? Hint, your audio files are 10 seconds long while the VGGish model receives an input of around 1 second audio.\n",
    "\n",
    "* 10: ?\n",
    "* 1: ?\n",
    "* 96: ?\n",
    "* 64: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the spectrogram of one example\n",
    "# We use numpy \"concatenate\" to join the 1-second chunks back together.\n",
    "plt.imshow(np.concatenate(np.concatenate(example_np)).T, aspect='auto', origin='lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, we can analyse the same audio file with librosa\n",
    "y, sr = librosa.load(\"%s/%s.wav\" % (audiofilefolder, fkeys[0]))\n",
    "example_librosa = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "example_librosa = librosa.power_to_db(example_librosa, ref=np.max)\n",
    "plt.imshow(example_librosa, aspect='auto', origin='lower')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE: look at the librosa documentation to find how to modify the options in the mel spectrogram calculation, and make this spectrogram match the VGGish one as close as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start to **perform some analysis** on the feature data. We'll start by analysing the spectrograms and how they relate to the class labels.\n",
    "\n",
    "This is a binary dataset: each audio file is labelled with **0** if there are no birds audible, and **1** if there is at least one bird audible.\n",
    "\n",
    "Does the spectrogram data contain sufficient information to decide automatically if a particular file should be 0 or 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (principal components analysis)\n",
    "\n",
    "# Note that we will collapse the \"frames\" axis so that each one-second chunk is represented by a single vector,\n",
    "# i.e. an average spectral profile for that one second.\n",
    "# We do this by taking the mean.\n",
    "\n",
    "# We're going to loop over the spectrograms, and the binary labels, in parallel,\n",
    "# so that we're sure they're in the same ordering.\n",
    "\n",
    "binarylabels_serial = []\n",
    "Xforpca = []\n",
    "for akey in fkeys:\n",
    "        if akey in melspecs:\n",
    "                aspec = melspecs[akey].detach().numpy()\n",
    "                aspec = aspec.mean(axis=(1,2))    # Here is where we apply that time-averaging for each clip.\n",
    "                if aspec.shape != (10,64):\n",
    "                        print(\"  warning: skipping item %s because melspec is unexpected shape\" % akey)\n",
    "                        continue\n",
    "                binarylabels_serial.extend([binarylabels[akey]] * aspec.shape[0])\n",
    "                Xforpca.append(aspec)\n",
    "\n",
    "\n",
    "Xforpca = np.array(Xforpca).reshape((-1, 64))\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pcadata = pca.fit_transform(Xforpca) # shape (nsamples, nfeatures)\n",
    "\n",
    "assert len(binarylabels_serial) == Xforpca.shape[0], \"len(binarylabels_serial) != Xforpca.shape[0]. %i != %i\" % (len(binarylabels_serial), Xforpca.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This convenience function will help us plot\n",
    "def scatterplot(xs, ys, title, datalabels):\n",
    "    plt.figure(frameon=False, figsize=(5, 5))\n",
    "    plt.scatter(xs, ys, alpha=0.2,\n",
    "                c=[{0: 'b', 1: 'g'}[albl] for albl in datalabels],\n",
    "               )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(pcadata[:,0], pcadata[:,1], \"2D PCA of mel spectrograms\", binarylabels_serial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 2: Does this plot give an impression that the two classes can be separated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's put the data through the VGGish network.\n",
    "\n",
    "# This may take a little while (10 seconds), or longer if the library has not yet downloaded VGGish.\n",
    "embedding_model = vggish(preprocess=False)\n",
    "embeddings = {key:embedding_model.forward(item).detach().numpy() for key,item in sorted(melspecs.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG creates a 128-dimensional vector (\"embedding\") for each spectrogram chunk.\n",
    "print(embeddings[fkeys[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, let's apply PCA so we can at least partly visualise the 128D outcome.\n",
    "\n",
    "binarylabels_serial = []\n",
    "Xforpca_vgg = []\n",
    "for akey in fkeys:\n",
    "        if akey in embeddings:\n",
    "                if embeddings[akey].shape != (10,128):\n",
    "                        print(\"  warning: skipping item %s because embedding is unexpected shape\" % akey)\n",
    "                        continue\n",
    "                binarylabels_serial.extend([binarylabels[akey]] * embeddings[akey].shape[0])\n",
    "                Xforpca_vgg.append(embeddings[akey])\n",
    "\n",
    "\n",
    "Xforpca_vgg = np.array(Xforpca_vgg).reshape((-1, 128))\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pcadata_vgg = pca.fit_transform(Xforpca_vgg) # shape (nsamples, nfeatures)\n",
    "\n",
    "assert len(binarylabels_serial) == Xforpca_vgg.shape[0], \"len(binarylabels_serial) != Xforpca.shape[0]. %i != %i\" % (len(binarylabels_serial), Xforpca.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot(pcadata_vgg[:,0], pcadata_vgg[:,1], \"2D PCA of VGG embeddings\", binarylabels_serial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 3: Does this plot give an impression that the two classes can be separated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESION 4: Why PCA is not necessarily a good idea for this analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate this by performing *logistic regression* on the data, instead of PCA. Logistic regression is a supervised learning algorithm, so it will use the class labels to learn its projection.\n",
    "\n",
    "Logistic regression is a lot like a single layer of a typical neural network. (Or, an extremely \"shallow\" neural network with no hidden layers between input and output.) If we perform logistic regression on the output of the VGG network, it's a bit like we're creating a new deep model with many layers, except most of the layers are pretrained, and only the last layer has been trained for our bird detection task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression applied directly to the mel spectrograms\n",
    "lgr = LogisticRegression(solver='liblinear')\n",
    "lgr.fit(Xforpca, binarylabels_serial)\n",
    "lgrdata = lgr.predict_proba(Xforpca) # shape (nsamples, nfeatures)\n",
    "lgrscore = lgr.score(Xforpca, binarylabels_serial) * 100\n",
    "\n",
    "scatterplot(lgrdata[:,0], range(len(lgrdata)), \"Logistic regression of mel spectrograms (accuracy: %.1f %%)\" % lgrscore, binarylabels_serial)\n",
    "plt.ylabel(\"Audio clip index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression applied to the VGG embeddings\n",
    "lgr = LogisticRegression(solver='liblinear')\n",
    "lgr.fit(Xforpca_vgg, binarylabels_serial)\n",
    "lgrdata_vgg = lgr.predict_proba(Xforpca_vgg) # shape (nsamples, nfeatures)\n",
    "lgrscore_vgg = lgr.score(Xforpca_vgg, binarylabels_serial) * 100\n",
    "\n",
    "scatterplot(lgrdata_vgg[:,0], range(len(lgrdata)), \"Logistic regression of VGG embeddings (accuracy: %.1f %%)\" % lgrscore_vgg, binarylabels_serial)\n",
    "plt.ylabel(\"Audio clip index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used scikit-learn's \"score\" function to calculate the accuracy of the logistic regression when used as a classifier. (Printed into the titles on the plots.) \n",
    "\n",
    "QUESTION 5: Do these accuracies reflect the differences you see (or don't see) in the scatter plots? Why / why not?\n",
    "\n",
    "QUESTION 6: What could be done further to improve the accuracy of classification, using a VGG-based model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
